{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs5NF0S3w4zX"
      },
      "source": [
        "# Inception v3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "hzNrZei8ojOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGCCWq0zw4zZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import models\n",
        "import torchvision.utils\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from timm.loss import LabelSmoothingCrossEntropy\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import sys\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao45IAvqw4za"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7UGY3A1w4za"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9pHS-ETw4zb"
      },
      "outputs": [],
      "source": [
        "# Data augementation pipline\n",
        "transform = transforms.Compose([transforms.RandomHorizontalFlip(0.9),\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(), # ToTensor : [0, 255] -> [0, 1]\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "​\n",
        "# Train transformation pipeline\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(), # ToTensor : [0, 255] -> [0, 1]\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "​\n",
        "# Test transformation pipeline\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(), # ToTensor : [0, 255] -> [0, 1]\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "train_data1 = dsets.ImageFolder('/content/drive/MyDrive/train/', transform)\n",
        "test_data1 = dsets.ImageFolder('/content/drive/MyDrive/val/',  transform)\n",
        "train_data2 = dsets.ImageFolder('/content/drive/MyDrive/train/', train_transform)\n",
        "test_data2 = dsets.ImageFolder('/content/drive/MyDrive/val/', test_transform)\n",
        "\n",
        "train_data = torch.utils.data.ConcatDataset([train_data1,train_data2])\n",
        "test_data = torch.utils.data.ConcatDataset([test_data1,test_data2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRuur64Jw4zb"
      },
      "outputs": [],
      "source": [
        "batch_size = 11\n",
        "\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True, drop_last=True)\n",
        "\n",
        "test_loader = DataLoader(test_data,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_len = len(train_data)\n",
        "test_data_len = len(test_data)\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\": test_loader\n",
        "}\n",
        "dataset_sizes = {\n",
        "    \"train\": train_data_len,\n",
        "    \"val\": test_data_len\n",
        "}"
      ],
      "metadata": {
        "id": "ind4NZmdJTjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_-AaGdHw4zc"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sUgC_Grw4zc"
      },
      "outputs": [],
      "source": [
        "model = models.inception_v3(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "NanPLWQDw4zc"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3gpN2faw4zd"
      },
      "outputs": [],
      "source": [
        "# Set parameters trainable/non-trainable\n",
        "model.aux_logits = False\n",
        "\n",
        "for parameter in model.parameters():\n",
        "    parameter.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To be used when fine-tuning on Batch Norm\n",
        "for parameter in model.parameters():\n",
        "    parameter.requires_grad = True\n",
        "    if isinstance(parameter, nn.BatchNorm2d):\n",
        "        parameter.requires_grad = False"
      ],
      "metadata": {
        "id": "pSK9JrEKpEDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively replace batch norm with layer norm\n",
        "def convert_layers(model, layer_type_old, layer_type_new, convert_weights=True, num_groups=None):\n",
        "    for name, module in reversed(model._modules.items()):\n",
        "        if len(list(module.children())) > 0:\n",
        "            # recurse\n",
        "            model._modules[name] = convert_layers(module, layer_type_old, layer_type_new, convert_weights)\n",
        "\n",
        "        if type(module) == layer_type_old:\n",
        "            layer_old = module\n",
        "            layer_new = layer_type_new(module.num_features if num_groups is None else num_groups, module.num_features, module.eps, module.affine)\n",
        "\n",
        "            if convert_weights:\n",
        "                layer_new.weight = layer_old.weight\n",
        "                layer_new.bias = layer_old.bias\n",
        "\n",
        "            model._modules[name] = layer_new\n",
        "\n",
        "    return model\n",
        "\n",
        "model = convert_layers(model, torch.nn.BatchNorm2d, torch.nn.GroupNorm, num_groups = 4)\n",
        "\n"
      ],
      "metadata": {
        "id": "cWX6VVD_Fq3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# When fine-tuning with Group Norm\n",
        "for parameter in model.parameters():\n",
        "    parameter.requires_grad = True\n",
        "    if isinstance(parameter, nn.GroupNorm):\n",
        "        parameter.requires_grad = False\n"
      ],
      "metadata": {
        "id": "zLWNkK5rTkth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ08LzDvw4zd"
      },
      "outputs": [],
      "source": [
        "# Fully connected classifier head\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(model.fc.in_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(512, 4)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO1ie_nXw4zd"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX_lQEwEw4zd"
      },
      "outputs": [],
      "source": [
        "loss_c = LabelSmoothingCrossEntropy()\n",
        "optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0045)\n",
        "num_epochs = 30\n",
        "#exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbSTBt-ww4zd"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "D3cV65ddw4zd"
      },
      "outputs": [],
      "source": [
        "def train_model(model, loss_c, optimizer, scheduler, num_epochs=50):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print(\"-\"*10)\n",
        "\n",
        "        for phase in ['train', 'val']: # We do training and validation phase per epoch\n",
        "            if phase == 'train':\n",
        "                model.train() # model to training mode\n",
        "            else:\n",
        "                model.eval() # model to evaluate\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "\n",
        "            for inputs, labels in tqdm(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'): # no autograd makes validation go faster\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1) # used for accuracy\n",
        "                    loss = loss_c(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step() # step at end of epoch\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc =  running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict()) # keep the best validation accuracy model\n",
        "        print()\n",
        "    time_elapsed = time.time() - since # slight error\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = train_model(model, loss_c, optimizer, exp_lr_scheduler)"
      ],
      "metadata": {
        "id": "qn23bbBJIRPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07wksPUew4zd"
      },
      "source": [
        "## Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKPA2KSDw4zd"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for images, labels in test_loader:\n",
        "\n",
        "    images = images.cuda()\n",
        "    outputs = model(images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels.cuda()).sum()\n",
        "\n",
        "print('Accuracy of test images: %f %%' % (100 * float(correct) / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "mBH_eW8H7Cpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "# iterate over test data\n",
        "for images, labels in train_loader:\n",
        "\n",
        "        images = images.cuda()\n",
        "        output = model(images) # Feed Network\n",
        "\n",
        "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
        "        y_pred.extend(output) # Save Prediction\n",
        "\n",
        "        labels = labels.data.cpu().numpy()\n",
        "        y_true.extend(labels) # Save Truth\n",
        "\n",
        "# constant for classes\n",
        "classes = (\"High_risk\", \"Low_risk\",\"Medium_risk\",\"invalid\")\n",
        "\n",
        "# Build confusion matrix\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n",
        "                     columns = [i for i in classes])\n",
        "plt.figure(figsize = (12,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.savefig('output.png')"
      ],
      "metadata": {
        "id": "xY9f1nVORT8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ2KqThyw4ze"
      },
      "outputs": [],
      "source": [
        "classes = [\"High_risk\", \"Medium_risk\",\"Low_risk\", \"invalid\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayWfJ5Xax_xC",
        "outputId": "47884119-f604-41d4-9bfb-920517645e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggsMnBuHw4ze"
      },
      "outputs": [],
      "source": [
        "images, labels = iter(test_loader).next()\n",
        "\n",
        "outputs = model(images.cuda())\n",
        "\n",
        "_, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(5)))\n",
        "\n",
        "title = (' '.join('%5s' % classes[labels[j]] for j in range(5)))\n",
        "imshow(torchvision.utils.make_grid(images, normalize=True), title)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}